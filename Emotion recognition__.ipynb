{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Recognition\n",
    "\n",
    "### Introduction to CNN Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **1.Introduction**\n",
    "- **2.Data preparation**\n",
    "   - Load data\n",
    "   - Check for null and missing values\n",
    "   - Normalization\n",
    "   - Reshape\n",
    "   - Label encoding\n",
    "   - Split training and testing set\n",
    "- **3.CNN**\n",
    "   - Define the model\n",
    "   - Set the optimizer\n",
    "   - Data augmentation\n",
    "- **4.Evaluate the model**\n",
    "   -  Training and validation curves\n",
    "- **5.Prediction and submition**\n",
    "   - Predict and Submit results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this Notebook, I built my first CNN for emotion recognition. I choosed to build it with keras API (Tensorflow backend) which is very intuitive. Firstly, I will prepare the data then i will focus on the CNN modeling and evaluation. \n",
    "\n",
    "- _This Notebook follows three main parts_ :\n",
    "   - The data preparation\n",
    "   - The CNN modeling and evaluation\n",
    "   - The results prediction and submission\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **About the dataset** :\n",
    "- The data consists of 48*48 pixel grayscale images of faces. The faces have been automatically registred so that the face is more or less concentered and occupies about the same amount of spaces in each images\n",
    "- The task is to categorize each face based on the emotion shown in the facial expression in to one of seven categories\n",
    "- Emotion column :\n",
    "     - 0 = Angry     \n",
    "     - 1 = Disgust\n",
    "     - 2 = Fear\n",
    "     - 3 = Happy\n",
    "     - 4 = Sad \n",
    "     - 5 = Surprise\n",
    "     - 6 = Neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop, Adagrad \n",
    "from keras.utils.np_utils import to_categorical # to covert to OneHotEncoder\n",
    "from keras.preprocessing.image import ImageDataGenerator #for data Augmentation\n",
    "from keras.regularizers import l2 \n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('fer2013.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     Usage\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
       "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
       "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
       "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Peck at the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Target variable is emotion\n",
    "- We notice that the pixels column contains all the 48*48 pixels. So as preprocessing, We have to split pixels column \n",
    "\n",
    "- Usage column will help us to extract both the train and test data from the raw data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35887 entries, 0 to 35886\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   emotion  35887 non-null  int64 \n",
      " 1   pixels   35887 non-null  object\n",
      " 2   Usage    35887 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 841.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35887, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data size\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for null and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion    0\n",
       "pixels     0\n",
       "Usage      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is no missing values in the dataset. So we can safely go ahead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Training       28709\n",
       "PrivateTest     3589\n",
       "PublicTest      3589\n",
       "Name: Usage, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Usage.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    8989\n",
       "6    6198\n",
       "4    6077\n",
       "2    5121\n",
       "0    4953\n",
       "5    4002\n",
       "1     547\n",
       "Name: emotion, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.emotion.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into the train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = [],[],[],[]\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    val = row['pixels'].split(\" \")\n",
    "    try:\n",
    "        if 'Training' in row['Usage']:\n",
    "            X_train.append(np.array(val, 'float32'))\n",
    "            y_train.append(row['emotion'])\n",
    "        elif 'PublicTest' in row['Usage']:\n",
    "            X_test.append(np.array(val, 'float32'))\n",
    "            y_test.append(row['emotion'])\n",
    "    except:\n",
    "        print(f'Error occured at index :{index} and row :{row}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train sample data \n",
      " :[array([ 70.,  80.,  82., ..., 106., 109.,  82.], dtype=float32), array([151., 150., 147., ..., 193., 183., 184.], dtype=float32)]\n",
      "-------------------------------\n",
      "Y_train sample data \n",
      ":[0, 0]\n",
      "-------------------------------\n",
      "X_test sample data \n",
      ":[array([254., 254., 254., ...,  42., 129., 180.], dtype=float32), array([156., 184., 198., ..., 172., 167., 161.], dtype=float32)]\n",
      "-------------------------------\n",
      "y_test sample data \n",
      ":[0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(f'X_train sample data \\n :{X_train[0:2]}')\n",
    "print('-------------------------------')\n",
    "print(f'Y_train sample data \\n:{y_train[0:2]}')\n",
    "print('-------------------------------')\n",
    "print(f'X_test sample data \\n:{X_test[0:2]}')\n",
    "print('-------------------------------')\n",
    "print(f'y_test sample data \\n:{y_test[0:2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Covert data into an array\n",
    "X_train = np.array(X_train, 'float32')\n",
    "y_train = np.array(y_train, 'float32')\n",
    "X_test  = np.array(X_test,'float32')\n",
    "y_test  = np.array(y_test,'float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We perform a grayscale normalization to reduce the effect of illumination's differences.\n",
    "- Moreover the CNN converg faster on [0..1] data than on [0..255]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data between 0 and 1\n",
    "#def NormalizeData(X):\n",
    "    X = X - np.mean(X, axis=0) / np.std(X, axis=0)\n",
    "    return X\n",
    "#X_train = NormalizeData(X_train)\n",
    "#X_test  = NormalizeData(X_test)\n",
    "\n",
    "#Normalisation des ds par Keras\n",
    "#x_train = tf.keras.utils.normalize(x_train, axis=1)\n",
    "#x_test = tf.keras.utils.normalize(x_test, axis=1)\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the pixel into (x_train[0],48,48,1)\n",
    "\n",
    "width, height = 48, 48\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], width, height, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], width, height, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train and test images (48px x 48px) has been stock into pandas.Dataframe as 1D vectors. We reshape all data to 48x48x1 3D matrices.\n",
    "- Keras requires an extra dimension in the end which correspond to channels. Emotion recognition images are gray scaled so it use only one channel. For RGB images, there is 3 channels, we would have reshaped 48*48px vectors to 28x28x3 3D matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, num_classes = 7)\n",
    "y_test  = to_categorical(y_test, num_classes = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have seven emotions from 0 to 6. We need to encode these lables to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0]), in order to compare it to the outpout (predicted value) which represents the probabilties of each labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28709, 7)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_test.shape\n",
    "#X_train.shape\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building The CNN :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I used the Keras Sequential API, where you have just to add one layer at a time, starting from the input.\n",
    "\n",
    "- The first is the convolutional (Conv2D) layer. It is like a set of learnable filters. I choosed to set 60 filters for the the first conv2D layer and 32 filters for the second one and so one so for . Each filter transforms a part of the image (defined by the kernel size) using the kernel filter. The kernel filter matrix is applied on the whole image. Filters can be seen as a transformation of the image.\n",
    "\n",
    "- The CNN can isolate features that are useful everywhere from these transformed images (feature maps).\n",
    "\n",
    "- I important also the pooling (MaxPool2D) layer. This layer simply acts as a downsampling filter. It looks at the 2 neighboring pixels and picks the maximal value. These are used to reduce computational cost, and to some extent also reduce overfitting. We have to choose the pooling size (i.e the area size pooled each time) more the pooling dimension is high, more the downsampling is important.\n",
    "\n",
    "- Combining convolutional and pooling layers, CNN are able to combine local features and learn more global features of the image.\n",
    "\n",
    "- Dropout is a regularization method, where a proportion of nodes in the layer are randomly ignored (setting their wieghts to zero) for each training sample. This drops randomly a propotion of the network and forces the network to learn features in a distributed way. This technique also improves generalization and reduces the overfitting.\n",
    "\n",
    "- 'relu' is the rectifier (activation function max(0,x). The rectifier activation function is used to add non linearity to the network.\n",
    "\n",
    "- The Flatten layer is use to convert the final feature maps into a one single 1D vector. This flattening step is needed so that you can make use of fully connected layers after some convolutional/maxpool layers. It combines all the found local features of the previous convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the CNN\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(60, kernel_size=(5,5),padding='same', activation='relu', input_shape=(width, height,1)))\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3,3),padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(60, kernel_size=(5,5),padding='same', activation='relu'))\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3,3),padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(60, kernel_size=(5,5),padding='same',activation='relu'))\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3,3),padding='same',activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "#1st Hidden Layer\n",
    "model.add(Dense(150, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "#2nd Hidden Layer\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "#3rd Hidden Layer\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "#Output\n",
    "model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Once our layers are added to the model, we need to set up a score function, a loss function and an optimisation algorithm.\n",
    "- We define the loss function to measure how poorly our model performs on images with known labels. It is the error rate between the oberved labels and the predicted ones. We use a specific form for categorical classifications (>2 classes) called the \"categorical_crossentropy\".\n",
    "- The most important function is the optimizer. This function will iteratively improve parameters (filters kernel values, weights and bias of neurons ...) in order to minimise the loss.\n",
    "- I choosed Adam , it is a very effective optimizer. The RMSProp update adjusts the Adagrad method in a very simple way in an attempt to reduce its aggressive, monotonically decreasing learning rate. We could also have used Stochastic Gradient Descent ('sgd') optimizer, but it is slower than RMSprop.\n",
    "- The metric function \"accuracy\" is used is to evaluate the performance our model. This metric function is similar to the loss function, except that the results from the metric evaluation are not used when training the model (only for evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 48, 48, 60)        1560      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 32)        17312     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 60)        48060     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 32)        17312     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 12, 12, 60)        48060     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 12, 32)        17312     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 150)               172950    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               15100     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7)                 357       \n",
      "=================================================================\n",
      "Total params: 343,073\n",
      "Trainable params: 343,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In order to avoid overfitting problem, we need to expand artificially our handwritten digit dataset. We can make your existing dataset even larger. The idea is to alter the training data with small transformations to reproduce the variations occuring when someone is writing a digit.\n",
    "- For example, the number is not centered The scale is not the same (some who write with big/small numbers) The image is rotated...\n",
    "- Approaches that alter the training data in ways that change the array representation while keeping the label the same are known as data augmentation techniques. Some popular augmentations people use are grayscales, horizontal flips, vertical flips, random crops, color jitters, translations, rotations, and much more.\n",
    "- By applying just a couple of these transformations to our training data, we can easily double or triple the number of training examples and create a very robust model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With data augmentation to prevent overfitting (accuracy 0.99286)\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For the data augmentation, i choosed to :\n",
    "\n",
    "   - Randomly rotate some training images by 10 degrees\n",
    "   - Randomly Zoom by 10% some training images\n",
    "   - Randomly shift images horizontally by 10% of the width\n",
    "   - Randomly shift images vertically by 10% of the height\n",
    "\n",
    "- I did not apply a vertical_flip nor horizontal_flip since it could have lead to misclassify symetrical numbers such as 6 and 9.\n",
    "- Once our model is ready, we fit the training dataset ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "898/898 [==============================] - 369s 411ms/step - loss: 1.8082 - accuracy: 0.2502\n",
      "Epoch 2/30\n",
      "898/898 [==============================] - 354s 395ms/step - loss: 1.7070 - accuracy: 0.3096\n",
      "Epoch 3/30\n",
      "898/898 [==============================] - 331s 369ms/step - loss: 1.6185 - accuracy: 0.3636\n",
      "Epoch 4/30\n",
      "898/898 [==============================] - 337s 376ms/step - loss: 1.5551 - accuracy: 0.3906\n",
      "Epoch 5/30\n",
      "898/898 [==============================] - 356s 396ms/step - loss: 1.5132 - accuracy: 0.4071\n",
      "Epoch 6/30\n",
      "898/898 [==============================] - 354s 394ms/step - loss: 1.4778 - accuracy: 0.4230\n",
      "Epoch 7/30\n",
      "898/898 [==============================] - 339s 378ms/step - loss: 1.4539 - accuracy: 0.4321\n",
      "Epoch 8/30\n",
      "898/898 [==============================] - 326s 363ms/step - loss: 1.4363 - accuracy: 0.4367\n",
      "Epoch 9/30\n",
      "898/898 [==============================] - 351s 390ms/step - loss: 1.4147 - accuracy: 0.4526\n",
      "Epoch 10/30\n",
      "898/898 [==============================] - 383s 426ms/step - loss: 1.4056 - accuracy: 0.4525\n",
      "Epoch 11/30\n",
      "898/898 [==============================] - 439s 489ms/step - loss: 1.3865 - accuracy: 0.4650\n",
      "Epoch 12/30\n",
      "898/898 [==============================] - 420s 467ms/step - loss: 1.3713 - accuracy: 0.4730\n",
      "Epoch 13/30\n",
      "898/898 [==============================] - 319s 356ms/step - loss: 1.3649 - accuracy: 0.4741\n",
      "Epoch 14/30\n",
      "898/898 [==============================] - 356s 397ms/step - loss: 1.3628 - accuracy: 0.4773\n",
      "Epoch 15/30\n",
      "898/898 [==============================] - 331s 369ms/step - loss: 1.3515 - accuracy: 0.4789\n",
      "Epoch 16/30\n",
      "898/898 [==============================] - 340s 379ms/step - loss: 1.3364 - accuracy: 0.4861\n",
      "Epoch 17/30\n",
      "898/898 [==============================] - 345s 384ms/step - loss: 1.3357 - accuracy: 0.4831\n",
      "Epoch 18/30\n",
      "898/898 [==============================] - 337s 375ms/step - loss: 1.3293 - accuracy: 0.4915\n",
      "Epoch 19/30\n",
      "898/898 [==============================] - 370s 412ms/step - loss: 1.3289 - accuracy: 0.4884\n",
      "Epoch 20/30\n",
      "898/898 [==============================] - 25601s 29s/step - loss: 1.3211 - accuracy: 0.4928\n",
      "Epoch 21/30\n",
      "898/898 [==============================] - 282s 314ms/step - loss: 1.3066 - accuracy: 0.5013\n",
      "Epoch 22/30\n",
      "898/898 [==============================] - 283s 316ms/step - loss: 1.3147 - accuracy: 0.5007\n",
      "Epoch 23/30\n",
      "898/898 [==============================] - 355s 395ms/step - loss: 1.2981 - accuracy: 0.5015\n",
      "Epoch 24/30\n",
      "898/898 [==============================] - 309s 344ms/step - loss: 1.3000 - accuracy: 0.5060\n",
      "Epoch 25/30\n",
      "898/898 [==============================] - 2056s 2s/step - loss: 1.2874 - accuracy: 0.5093\n",
      "Epoch 26/30\n",
      "898/898 [==============================] - 289s 321ms/step - loss: 1.2882 - accuracy: 0.5073\n",
      "Epoch 27/30\n",
      "898/898 [==============================] - 287s 320ms/step - loss: 1.2910 - accuracy: 0.5109\n",
      "Epoch 28/30\n",
      "898/898 [==============================] - 358s 399ms/step - loss: 1.2896 - accuracy: 0.5069\n",
      "Epoch 29/30\n",
      "898/898 [==============================] - 281s 313ms/step - loss: 1.2793 - accuracy: 0.5157\n",
      "Epoch 30/30\n",
      "898/898 [==============================] - 281s 313ms/step - loss: 1.2816 - accuracy: 0.5110\n",
      "CPU times: user 7h 32min 20s, sys: 56min 50s, total: 8h 29min 10s\n",
      "Wall time: 10h 19min 12s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd840420f10>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X_train, y_train, \n",
    "          batch_size=32,\n",
    "          epochs=30, \n",
    "          shuffle=True,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 8s 69ms/step - loss: 1.2275 - accuracy: 0.5336\n",
      "model Loss : 1.2274876832962036\n",
      "model Accuarcy : 0.5335748195648193\n"
     ]
    }
   ],
   "source": [
    "model_loss, model_acuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f'model Loss : {model_loss}')\n",
    "print(f'model Accuarcy : {model_acuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fer_json = model.to_json()\n",
    "with open('fer.json','w') as json_file:\n",
    "    json_file.write(fer_json)\n",
    "model.save_weights('fer.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/mac/Downloads/Real-Time-Emotion-Detection-main'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.getcwd()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
